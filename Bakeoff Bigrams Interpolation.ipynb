{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the necessary libraries\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "import pandas\n",
    "import copy\n",
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Read in the train.txt file\n",
    "\n",
    "fp = open('train.txt', 'r')\n",
    "train = fp.readlines()\n",
    "fp.close()\n",
    "\n",
    "#Number of sentences in train.txt that pertain to each word pair\n",
    "dist = [24200, 24200, 2890, 24200, 24200, 24200, 7269, 24200, 24200, 24200,\n",
    "       24200, 24200, 6068, 24200, 24200, 3396, 24200, 24200, 24200, 24200, \n",
    "       12105, 11497, 16563, 5049, 4534]\n",
    "\n",
    "#List of tuples of the word pairs\n",
    "pairs = [\n",
    "    (\"a\" , \"á\"),\n",
    "    (\"ais\", \"áis\"),\n",
    "    (\"aisti\", \"aistí\"),\n",
    "    (\"ait\", \"áit\"),\n",
    "    (\"ar\", \"ár\"),\n",
    "    (\"arsa\", \"ársa\"),\n",
    "    (\"ban\", \"bán\"),\n",
    "    (\"cead\", \"céad\"),\n",
    "    (\"chas\", \"chás\"),\n",
    "    (\"chuig\", \"chúig\"),\n",
    "    (\"dar\", \"dár\"),\n",
    "    (\"do\", \"dó\"),\n",
    "    (\"gaire\", \"gáire\"),\n",
    "    (\"i\", \"í\"),\n",
    "    (\"inar\", \"inár\"),\n",
    "    (\"leacht\", \"léacht\"),\n",
    "    (\"leas\", \"léas\"),\n",
    "    (\"mo\", \"mó\"),\n",
    "    (\"na\", \"ná\"),\n",
    "    (\"os\", \"ós\"),\n",
    "    (\"re\", \"ré\"),\n",
    "    (\"scor\", \"scór\"),\n",
    "    (\"te\", \"té\"),\n",
    "    (\"teann\", \"téann\"),\n",
    "    (\"thoir\", \"thóir\")\n",
    "]\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code to label each sentence in train.txt as either accented or ascii\n",
    "\n",
    "pairsindex = 0\n",
    "count = 0\n",
    "\n",
    "asciioraccent = []\n",
    "sector = []\n",
    "\n",
    "sentences = []\n",
    "sentencesector = []\n",
    "\n",
    "\n",
    "for t in train:\n",
    "    if count == dist[pairsindex]:\n",
    "        #print(pairs[pairsindex][0], pairs[pairsindex][1])\n",
    "        pairsindex += 1\n",
    "        asciioraccent.append(copy.deepcopy(sector))\n",
    "        sentences.append(copy.deepcopy(sentencesector))\n",
    "        #print(sector[1:50])\n",
    "        sector.clear()\n",
    "        sentencesector.clear()\n",
    "        count = 0\n",
    "    sentence = re.compile('\\w+').findall(t)\n",
    "    for s in sentence:\n",
    "        if s == pairs[pairsindex][0] or s == pairs[pairsindex][1]:\n",
    "            target = s\n",
    "            if target == pairs[pairsindex][0]:\n",
    "                sector.append(1)\n",
    "            else:\n",
    "                sector.append(0)\n",
    "            sentencesector.append(t)\n",
    "            break\n",
    "    count += 1\n",
    "\n",
    "sentences.append(copy.deepcopy(sentencesector))\n",
    "asciioraccent.append(copy.deepcopy(sector))\n",
    "\n",
    "sum = 0\n",
    "for a in asciioraccent:\n",
    "    sum += len(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting training set into training and validation sets\n",
    "\n",
    "training = []\n",
    "val = []\n",
    "answers = []\n",
    "\n",
    "sum = 0\n",
    "sum2 = 0\n",
    "sum3 = 0\n",
    "\n",
    "for i in range(0, 25):\n",
    "    trainer, validate, ignore, answer = train_test_split(sentences[i], asciioraccent[i])\n",
    "    training.append(trainer)\n",
    "    val.append(validate)\n",
    "    answers.append(answer)\n",
    "    sum += len(trainer)\n",
    "    sum2 += len(validate)\n",
    "    sum3 += len(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping all the words in train.txt into 25 big lists - one for each word pair\n",
    "\n",
    "distindex = 0\n",
    "count = 0\n",
    "\n",
    "wordclumps = []\n",
    "wordclumpdist = []\n",
    "\n",
    "for t in training:\n",
    "    for q in t:\n",
    "        sentence = re.compile('\\w+').findall(q)\n",
    "        for s in sentence:\n",
    "            wordclumpdist.append(s)\n",
    "    wordclumps.append(copy.deepcopy(wordclumpdist))\n",
    "    wordclumpdist.clear()        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training algorithm - this involves taking the unigram and bigram of each big list of \"wordclumps\", finding the bigrams\n",
    "#that have one of the words in the word pairs in them (bigrams that have either a or á, for example), and computing\n",
    "#probabilities for those\n",
    "\n",
    "pairsindex = 0\n",
    "    \n",
    "probs = {}\n",
    "\n",
    "unipair = []\n",
    "unigrams = []\n",
    "bigrams = []\n",
    "pertbigrams = []\n",
    "\n",
    "possiblepairs = []\n",
    "distinctseen = []\n",
    "totalseen = []\n",
    "unseen = []\n",
    "onepairs = []\n",
    "\n",
    "for wc in wordclumps:\n",
    "    \n",
    "    uni = list(Counter(wc).items())\n",
    "    bi = list(Counter(ngrams(wc, 2)).items())\n",
    "    \n",
    "    possible = len(uni) * len(uni)\n",
    "    distinct = len(bi)\n",
    "    \n",
    "    total = 0\n",
    "    for u in uni:\n",
    "        total += u[1]\n",
    "        \n",
    "    un = possible - distinct\n",
    "    \n",
    "    one = 0\n",
    "    for b in bi:\n",
    "        if b[1] == 1:\n",
    "            one += 1\n",
    "    \n",
    "    possiblepairs.append(possible)\n",
    "    distinctseen.append(distinct)\n",
    "    totalseen.append(total)\n",
    "    unseen.append(un)\n",
    "    onepairs.append(one)\n",
    "    \n",
    "    unigrams = uni\n",
    "    bigrams = bi\n",
    "    \n",
    "    \n",
    "    \n",
    "    for b in bigrams:\n",
    "        if pairs[pairsindex][0] in b[0] or pairs[pairsindex][1] in b[0]:\n",
    "            pertbigrams.append(b)\n",
    "    \n",
    "    for p in pertbigrams:\n",
    "        otherword = 0\n",
    "        if p[0][0] == pairs[pairsindex][0] or p[0][0] == pairs[pairsindex][1]:\n",
    "            for u in unigrams:\n",
    "                if u[0] == p[0][1]:\n",
    "                    otherword = u[1]\n",
    "                    break\n",
    "            if otherword == 0:\n",
    "                continue\n",
    "            else:\n",
    "                probs[p[0]] = p[1]/otherword\n",
    "               #print(p[0], p[1]/otherword)\n",
    "                continue\n",
    "        if p[0][1] == pairs[pairsindex][0] or p[0][1] == pairs[pairsindex][1]:\n",
    "            for u in unigrams:\n",
    "                if u[0] == p[0][0]:\n",
    "                    otherword = u[1]\n",
    "                    break\n",
    "            if otherword == 0:\n",
    "                continue\n",
    "            else:\n",
    "                probs[p[0]] = p[1]/otherword\n",
    "                #print(p[0], p[1]/otherword)\n",
    "                continue\n",
    "                \n",
    "    \n",
    "    for u in unigrams:\n",
    "        if pairs[pairsindex][0] in u or pairs[pairsindex][1] in u:\n",
    "            unipair.append(u)\n",
    "    \n",
    "    pertbigrams = []\n",
    "    print(\"Iteration \" + str(pairsindex + 1) + \" complete\")\n",
    "    pairsindex += 1\n",
    "        \n",
    "\n",
    "    \n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dumping all the words in wordclumps into one really big list of words and taking unigrams and probabilities of that really\n",
    "#big list\n",
    "\n",
    "wordbag = []\n",
    "alluniprobs = {}\n",
    "\n",
    "for wc in wordclumps:\n",
    "    for w in wc:\n",
    "        wordbag.append(w)\n",
    "\n",
    "allwords = len(wordbag)\n",
    "allunigrams = list(Counter(wc).items())\n",
    "\n",
    "for a in allunigrams:\n",
    "    alluniprobs[a[0]] = a[1]/allwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute unigram probabilities for each of the word pairs\n",
    "\n",
    "uniwordprobs = {}\n",
    "\n",
    "pairsindex = 0\n",
    "distindex = 0\n",
    "count = 1\n",
    "\n",
    "asciicount = 0\n",
    "accentcount = 0\n",
    "\n",
    "for t in train:\n",
    "    count += 1\n",
    "    sentence = re.compile('\\w+').findall(t)\n",
    "    for s in sentence:\n",
    "        if s == pairs[pairsindex][0]:\n",
    "            asciicount += 1\n",
    "            break\n",
    "        elif s == pairs[pairsindex][1]:\n",
    "            accentcount += 1\n",
    "            break\n",
    "    if count == dist[distindex]:\n",
    "        count = 0\n",
    "        uniwordprobs[pairs[pairsindex][0]] = (asciicount/dist[distindex])\n",
    "        uniwordprobs[pairs[pairsindex][1]] = (accentcount/dist[distindex])\n",
    "        if distindex < 24:\n",
    "            distindex += 1\n",
    "        if pairsindex < 24:\n",
    "            pairsindex += 1\n",
    "        asciicount = 0\n",
    "        accentcount = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing word pair words in the sentences in the validation set with the {|} structure\n",
    "#E.g. replacing \"a\" in a sentence with {a|á}\n",
    "\n",
    "pairsindex = 0\n",
    "\n",
    "validation = []\n",
    "valsector = []\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for v1 in val:\n",
    "    decider = \"{\" + pairs[pairsindex][0] + \"|\" + pairs[pairsindex][1] + \"}\"\n",
    "    for v in v1:\n",
    "        sentence = re.compile('[\\w{}À-ÿ|]+').findall(v)\n",
    "        target = \"\"\n",
    "        for s in sentence:\n",
    "            if s == pairs[pairsindex][0] or s == pairs[pairsindex][1]:\n",
    "                target = s\n",
    "                break\n",
    "        concat = ' '.join(sentence)\n",
    "        sub = \"\\\\b\" + target + \"\\\\b\"\n",
    "        valsector.append(re.sub(sub, decider, concat).strip('\\n'))\n",
    "    sum += len(valsector)\n",
    "    validation.append(copy.deepcopy(valsector))\n",
    "    valsector.clear()\n",
    "    pairsindex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training interpolation hyperparameters (lambdas) on the validation set\n",
    "\n",
    "pairsindex = 0\n",
    "guesses = []\n",
    "allanswers = []\n",
    "\n",
    "\n",
    "for an in answers:\n",
    "    for a in an:\n",
    "        allanswers.append(a)\n",
    "\n",
    "#front bigram\n",
    "lamb = 0.35\n",
    "\n",
    "#front unigram\n",
    "lamb2 = 0.2\n",
    "\n",
    "#back bigram\n",
    "lamb3 = 0.35\n",
    "\n",
    "#back unigram\n",
    "lamb4 = 0.1\n",
    "\n",
    "sum = 0\n",
    "for vi in validation:\n",
    "    sum += len(vi)\n",
    "\n",
    "#print(sum)\n",
    "\n",
    "reps = 0\n",
    "    \n",
    "#Cycle through the validation set, find the {|} structure, and use interpolation to calculate guesses based off of the\n",
    "#previous word and the next word\n",
    "for vi in validation:\n",
    "    decide = \"{\" + pairs[pairsindex][0] + \"|\" + pairs[pairsindex][1] + \"}\"\n",
    "    for v in vi:\n",
    "        sentence = re.compile('[\\w{}À-ÿ|]+').findall(v)\n",
    "        for s in sentence:\n",
    "            if s == decide:\n",
    "                #find previous word and next word\n",
    "                prevword, nextword = None, None\n",
    "                index = sentence.index(decide)\n",
    "                if index > 0:\n",
    "                    prevword = sentence[index - 1]\n",
    "                if index < (len(sentence) - 1):\n",
    "                    nextword = sentence[index + 1]\n",
    "                \n",
    "                #calculate alpha and beta according to interpolation\n",
    "                alpha = 0\n",
    "                beta = 0\n",
    "\n",
    "                afront = 0.0\n",
    "                aback = 0.0\n",
    "                bfront = 0.0\n",
    "                bback = 0.0\n",
    "\n",
    "                afront = lamb * probs.get((prevword, pairs[pairsindex][0]), 0.0) \\\n",
    "                + lamb2 * uniwordprobs.get(pairs[pairsindex][0], 0.0)\n",
    "                \n",
    "                aback = lamb3 * probs.get((pairs[pairsindex][0], nextword), 0.0) \\\n",
    "                + lamb4 * alluniprobs.get(nextword, 0.0)\n",
    "\n",
    "                bfront = lamb * probs.get((prevword, pairs[pairsindex][1]), 0.0) \\\n",
    "                + lamb2 * uniwordprobs.get(pairs[pairsindex][1], 0.0)\n",
    "                \n",
    "                bback = lamb3 * probs.get((pairs[pairsindex][1], nextword), 0.0) \\\n",
    "                + lamb4 * alluniprobs.get(nextword, 0.0)\n",
    "\n",
    "                alpha =  afront * aback\n",
    "                beta =  bfront * bback\n",
    "                if alpha != 0 and beta != 0:\n",
    "                    guesses.append(alpha/(alpha + beta))\n",
    "                else:\n",
    "                    uniprob = uniwordprobs.get(pairs[pairsindex][0], 0.0)\n",
    "                    guesses.append(uniprob)\n",
    "                reps += 1\n",
    "                break\n",
    "    pairsindex += 1\n",
    "    \n",
    "\n",
    "#Take log loss in order to improve hyperparameters    \n",
    "print(log_loss(allanswers, guesses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing - same algorithm as previous cell except it's for the test set\n",
    "fq = open('test.txt', 'r')\n",
    "test = fq.readlines()\n",
    "fq.close()\n",
    "\n",
    "probssub = []\n",
    "\n",
    "\n",
    "count = 0\n",
    "pairsindex = 0\n",
    "\n",
    "stragglers = []\n",
    "\n",
    "#print(uniwordprobs)\n",
    "\n",
    "for t in test:\n",
    "    if count == 800:\n",
    "        count = 0\n",
    "        if pairsindex < 24:\n",
    "            pairsindex += 1\n",
    "    sentence = re.compile('[\\w{}À-ÿ|]+').findall(t)\n",
    "    decide = \"{\" + pairs[pairsindex][0] + \"|\" + pairs[pairsindex][1] + \"}\"\n",
    "    for s in sentence:\n",
    "        if s == decide:\n",
    "            prevword, nextword = None, None\n",
    "            index = sentence.index(decide)\n",
    "            if index > 0:\n",
    "                prevword = sentence[index - 1]\n",
    "            if index < (len(sentence) - 1):\n",
    "                nextword = sentence[index + 1]\n",
    "            alpha = 0\n",
    "            beta = 0\n",
    "            \n",
    "            afront = 0.0\n",
    "            aback = 0.0\n",
    "            bfront = 0.0\n",
    "            bback = 0.0\n",
    "\n",
    "            afront = lamb * probs.get((prevword, pairs[pairsindex][0]), 0.0) \\\n",
    "            + lamb2 * uniwordprobs.get(pairs[pairsindex][0], 0.0)\n",
    "                \n",
    "            aback = lamb3 * probs.get((pairs[pairsindex][0], nextword), 0.0) \\\n",
    "            + lamb4 * alluniprobs.get(nextword, 0.0)\n",
    "\n",
    "            bfront = lamb * probs.get((prevword, pairs[pairsindex][1]), 0.0) \\\n",
    "            + lamb2 * uniwordprobs.get(pairs[pairsindex][1], 0.0)\n",
    "                \n",
    "            bback = lamb3 * probs.get((pairs[pairsindex][1], nextword), 0.0) \\\n",
    "            + lamb4 * alluniprobs.get(nextword, 0.0)\n",
    "\n",
    "            alpha =  afront * aback\n",
    "            beta =  bfront * bback\n",
    "            if alpha != 0 and beta != 0:\n",
    "                probssub.append(alpha/(alpha + beta))\n",
    "            else:\n",
    "                uniprob = uniwordprobs.get(pairs[pairsindex][0], 0.0)\n",
    "                probssub.append(uniprob)\n",
    "                \n",
    "                \n",
    "            break\n",
    "    count += 1\n",
    "    \n",
    "pairsindex = 0\n",
    "       \n",
    "print(\"Testing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing guesses to csv file for submission\n",
    "df = pandas.read_csv('sampleSubmission.csv')\n",
    "\n",
    "count = 0\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Expected'] = probssub[count]\n",
    "    count += 1\n",
    "        \n",
    "df.to_csv('Submission.csv', index = False)\n",
    "\n",
    "print(\"Writing complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
